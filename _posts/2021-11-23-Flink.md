---
layout: post
title: "FLINK"
date: 2021-11-24
description: "FLINK"
tag: hexo
---   
## 简介
    Apache Flink是一个高性能的分布式流处理框架，并且实现了批处理。 Flink定义了窗口的概念，一个窗口即包含一定的数据段，支持4种窗口：

    时间窗口，窗口包含一定时间段中的所有数据，最简单也最有用的窗口，支持滑动（一次移动自定义的时间）和滚动（一次移动整个窗口大小的时间）。
    计数窗口，窗口包含特定数量的数据，但可能会因为没有数据来源而浪费资源。
    会话窗口，会话即活动，其时间不定，数据不定，可通过超时来关闭窗口。
    触发器，前3中窗口均包含特定的触发器，也可自定义触发器来实现专用的窗口，比如带超时的计数窗口。 在时间窗口中，关闭时通过数据流中的水印字段实现的，水印字段是有程序开发人员生成，而不是Flink本身，因为不同领域情况不一样。
## Flink API

    对于数据流来说，核心数据结构为xxxStream, 比如DataStream, KeyedStream等。

## 数据转换

map - DataStream -> DataStream 将数据流一对一的转换
    
    // 字符串(String) -> 字符串长度(Integer)
    env.fromElements("name", "mark").map(value -> {
    return value.length();
    });

flatMap - DataStream -> DataStream 数据流一对多转换
    
    // 字符串(String) -> 字符串(String)
    env.fromElements("name,mark,test").map((v, c) -> {
    for (String str : v.split(",")) {
    c.collect(str);
    }
    }).returns(String.class)
    });

flater - DataStream -> DataStream 数据流过滤
    
    // 返回false即被过滤。
    dataStream.flater(v -> {
    if (...) {
    return true;
    } else {
    return false;
    }
    })
    });

keyBy - DataStream -> KeyedStream 数据分类，安装特定的规则将数据进行分类，然后使用reduce、sum、mix、min等进行聚合
    
    // 按Name字段分类， 在统计val总和。
    dataStream.keyBy(v -> {
    return v.Name;
    }).reduce((v1, v2) -> {
    return v1.val + v2.val;
    })
    });

process - KeyedStream -> SingleOutputStreamOperator 将按照Key分组后的数据流转换为单输出流，核心是通过一个KeyedProcessFunction来进行处理。
    
    // K - 分组的key的类型
    // V - 入数据流实际类型
    // O - 出数据流实际类型
    new KeyedProcessFunction<K,V,O>() {
    // 数据流的状态，每一个分组的Key映射一个value，可以根据使用state.value()获取到对应的状态。
    ValueState<I> state;
    // 每次创建KeyedProcessFunction时调用，和并行度有关，每个线程创建一个。
    // 可以用于创建比较费资源、可复用的的对象。
    @Override
    public void open(Configuration parameters) throws Exception {
    // 初始化状态
    ValueStateDescriptor<String> desc = new ValueStateDescriptor<>("state-name", I.class);
    RuntimeContext rCtx = getRuntimeContext();
    state = rCtx.getState(desc);
    super.open(parameters);
    }
    @Override
    public void processElement(V value, Context ctx, Collector<O>> out) throws Exception {
    // 从RuntimeContext获取currentKey，在根据currentKey获取对应的Value。
    String stateValue = state.value();
    // stateValue 为 null 或者 默认值，则为该分组的第一个
    // 否则需要根据自定义逻辑判断
    // TODO Process
    }
    // 销毁时关闭，可用于关闭链接等。
    @Override
    public void close() {
    }
    }

## 连接kafka
 pom依赖，
    
    kafka 依赖
    
    <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-core</artifactId>
        <!-- 本机调试使用这个 -->
        <!-- <scope>compile</scope> -->
        <scope>provided</scope>
    </dependency>
    <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-streaming-java_2.11</artifactId>
        <version>1.9.0</version>
    </dependency>


 args由实时计算平台运行时传入

// 本地调试可用：-jobName jobOfLocalTest --_jobId jobOfLocalTest01 --_offset_zk lion-zk.test.vip.sankuai.com --local true

    public static void kafka2Flink(String[] args) throws Exception {
        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setRestartStrategy(new RestartStrategies.NoRestartStrategyConfiguration());
        FlinkkafkaConsumer.Builder<String> cBuilder = FlinkkafkaConsumer.newBuilder();
        cBuilder.consumerGroup(TOPIC, SUB_GROUP, NAMESPACE, APPKEY)
                .setRtProperties(args)
                .startFromLatest() // 
                .deserializationSchema(new kafkaSimpleStringSchema());
        FlinkkafkaConsumer consumer = cBuilder.build();
        DataStream<String> source = env.addSource(consumer).shuffle();
        // process chain
        source.print();
        env.execute();
}